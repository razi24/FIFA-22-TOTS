
import pandas as pd
import csv
import os
import bs4
from bs4 import BeautifulSoup
import numpy as np
from requests.api import head
import scipy as sc
import requests
url="https://www.futwiz.com/en/fifa21/players?page=0"
page= requests.get(url)
soup = BeautifulSoup(page.content, 'html.parser')
file_name='FIFA21.csv'
f=open(file_name,'w')
csv_writer=csv.writer(f)

data=['Player','Team','League','Version']   #inserting headrs 
t_head=soup.find('tr',class_="head")
for th in t_head('td'):
    data.append(th.text.strip()) 
    
del data[4]
del data[4]


csv_writer.writerow(data)

for i in range(907 # for all pages in the website 
):
    
 
    for tr in soup.find_all('tr',class_="table-row"):  #Taking data from table rows 
        data=[]
        simple_data=[]


        for td in tr.find_all('td'):
        
            if( td.find_all('p')):
                p1=td.find('p',class_="name")
                data.append(p1.text.strip())
             


                
                p4=td.find('p',class_="team")
                    
            
                for x in p4.find_all('a'):
                    data.append(x.text)
                    #print(simple_data)


        
            

                
                
                
                

            else:
                if(td.find('span',class_='stat')):
                   x=td.find('span',class_='stat')
                   data.append(x.text)
                   
                else:
                    if(td.find('div',class_="otherversion21-txt")): # Getting the virsion of the card
                        x7=td.find('div')
                        x7=x7['class'][1]
                        x7=x7[x7.index("-"):]
                        x7=x7[1:]
                        data.append(x7)
                        x8=td.text.strip()
                        data.append(x8)

                    else:


                        text2=td.text.strip()
                        text2=text2.replace('\n',' ')
                        data.append(text2)         
        del data[0]


        if data:
            #print("inserting data:{}".format(','.join(data)))
            csv_writer.writerow(data)

    
    if(i<10):
        url=url[:-1]+str(i+1)
        page= requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser') 
    if((i>=10)&(i<100)):
        url=url[:-2]+str(i+1)
        page= requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser') 
    if(i>=100):
        url=url[:-3]+str(i+1)
        page= requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser')               

f.close()


