import pandas as pd
import os
import bs4
from bs4 import BeautifulSoup
import numpy as np
from pandas.io.formats import style
import scipy as sc
import requests
import csv



#url="https://www.futwiz.com/en/fifa20/players?page=0"
#pages=955
url="https://www.futwiz.com/en/fifa21/players?page=0"
pages=907
page= requests.get(url)
soup = BeautifulSoup(page.content, 'html.parser')
file_name='FIFA21.csv'
f=open(file_name,'w')
csv_writer=csv.writer(f)


# Writing the headers of the table
data=['Player','Version','Team','League']
t_head=soup.find('tr',class_="head")
for th in t_head('td'):
    data.append(th.text.strip()) 
    
del data[4]
del data[4]


csv_writer.writerow(data)


# Writing the data from all pages 

for i in range(907):
    
 
    for tr in soup.find_all('tr',class_="table-row"):
        data=[]
        simple_data=[]


        for td in tr.find_all('td'):
        
            if( td.find_all('p')):
                p1=td.find('p',class_="name")
                data.append(p1.text.strip())
                p2=p1.find('a')
                x=x[:x.index("|")]
                data.append(x)


                
                p4=td.find('p',class_="team")
                    
            
                for x in p4.find_all('a'):
                    data.append(x.text)
                    #print(simple_data)
                
                
                
                

            else:
                if(td.find('span',class_='stat')):
                   x=td.find('span',class_='stat')
                   data.append(x.text)
                else:

                    text2=td.text.strip()
                    text2=text2.replace('\n',' ')
                    data.append(text2)         
        del data[0]


        if data:
            
            csv_writer.writerow(data)

    # Changing the url to the next page 

    if(i<10):
        url=url[:-1]+str(i+1)
        page= requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser') 
    if((i>=10)&(i<100)):
        url=url[:-2]+str(i+1)
        page= requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser') 
    if(i>=100):
        url=url[:-3]+str(i+1)
        page= requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser')               

f.close()



